<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>Pedro Vicente | publications</title>
  <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

  <link rel="shortcut icon" href="/assets/img/iCub_pvicente_ISR.jpg">

  <link rel="stylesheet" href="/assets/css/main.css">
  <link rel="canonical" href="/publications/">
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    
    <span class="site-title">
        
        <strong>Pedro</strong> Vicente
    </span>
    

    <nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

      <div class="trigger">
        <!-- About -->
        <a class="page-link" href="/">about</a>

        <!-- Blog -->
        <!-- <a class="page-link" href="/blog/">blog</a> -->

        <!-- Pages -->
        
          
        
          
            <a class="page-link" href="/publications/">publications</a>
          
        
          
            <a class="page-link" href="/teaching/">teaching</a>
          
        
          
            <a class="page-link" href="/funding_awards/">funding&awards</a>
          
        
          
        
          
        
          
        
          
        

        <!-- CV link -->
        <!-- <a class="page-link" href="/assets/pdf/CV.pdf">vitae</a> -->

      </div>
    </nav>

  </div>

</header>



    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">publications</h1>
    <h5 class="post-description">Publications by categories in reversed chronological order. Generated by jekyll-scholar.</h5>
  </header>

  <article class="post-content publications clearfix">
    <h2 class="year">Journals</h2>

<h3 class="year">2020</h3>
<ol class="bibliography"><li>

<div id="kim2020jint">
  
    <span class="title"><b>Cleaning Tasks Knowledge Transfer Between Heterogeneous Robots: a
               Deep Learning Approach</b></span>
    <span class="author">
      
        
          
            
              Jaeseok Kim,
            
          
        
      
        
          
            
              Nino Cauli,
            
          
        
      
        
          
            <em>Pedro Vicente</em>,
          
        
      
        
          
            
              Bruno D. Damas,
            
          
        
      
        
          
            
              Alexandre Bernardino,
            
          
        
      
        
          
            
              José Santos-Victor,
            
          
        
      
        
          and
          
            
              Filippo Cavallo
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>Journal of Intelligent &amp; Robotic Systems</em>
    
    
      2020
    
    </span>
  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
  
  
  [<a href="http://doi.org/10.1007/s10846-019-01072-4" target="_blank">DOI</a>]

  
    [<a href="/assets/pdf/jkim_jint2020.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
    [<a class="bibtex">Bibtex</a>]
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Nowadays, autonomous service robots are becoming an important topic in robotic research. Differently from typical industrial scenarios, with highly controlled environments, service robots must show an additional robustness to task perturbations and changes in the characteristics of their sensory feedback. In this paper, a robot is taught to perform two different cleaning tasks over a table, using a learning from demonstration paradigm. However, differently from other approaches, a convolutional neural network is used to generalize the demonstrations to different, not yet seen dirt or stain patterns on the same table using only visual feedback, and to perform cleaning movements accordingly. Robustness to robot posture and illumination changes is achieved using data augmentation techniques and camera images transformation. This robustness allows the transfer of knowledge regarding execution of cleaning tasks between heterogeneous robots operating in different environmental settings. To demonstrate the viability of the proposed approach, a network trained in Lisbon to perform cleaning tasks, using the iCub robot, is successfully employed by the DoRo robot in Peccioli, Italy.</p>
  </span>
  

  
  <span class="bibtex hidden">
    <p>@article{kim2020jint,
  author = {Kim, Jaeseok and Cauli, Nino and Vicente, Pedro and Damas, Bruno D. and Bernardino, Alexandre and Santos{-}Victor, Jos{\'{e}} and Cavallo, Filippo},
  title = {Cleaning Tasks Knowledge Transfer Between Heterogeneous Robots: a
                 Deep Learning Approach},
  journal = {Journal of Intelligent {\&amp;} Robotic Systems},
  volume = {98},
  number = {1},
  pages = {191--205},
  year = {2020},
  url = {https://doi.org/10.1007/s10846-019-01072-4},
  doi = {10.1007/s10846-019-01072-4},
  pdf = {jkim_jint2020.pdf}
}
</p>
  </span>
  


  
</div>
</li></ol>

<h3 class="year">2018</h3>
<ol class="bibliography"><li>

<div id="10.3389/frobt.2018.00046">
  
    <span class="title"><b>Markerless Eye-Hand Kinematic Calibration on the iCub Humanoid Robot</b></span>
    <span class="author">
      
        
          
            <em>Pedro Vicente</em>,
          
        
      
        
          
            
              Lorenzo Jamone,
            
          
        
      
        
          and
          
            
              Alexandre Bernardino
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>Frontiers in Robotics and AI</em>
    
    
      2018
    
    </span>
  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
  
  
  [<a href="http://doi.org/10.3389/frobt.2018.00046" target="_blank">DOI</a>]

  
    [<a href="/assets/pdf/pvicente-frontiers2018.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
    [<a class="bibtex">Bibtex</a>]
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Humanoid robots are resourceful platforms and can be used in diverse application scenarios.However,  their  high  number  of  degrees  of  freedom  in  (i.e.,  moving  arms,  head  and  eyes) deteriorates the precision of eye-hand coordination.  A good kinematic calibration is often difficult to achieve, due to several factors, e.g., unmodeled deformations of the structure or backlash in the actuators. This is particularly challenging for very complex robots such as the iCub humanoid robot, which has 12 degrees of freedom and cable-driven actuation in the serial chain from the eyes to the hand. The exploitation of real-time robot sensing is of paramount importance to  increase  the  accuracy  of  the  coordination,  for  example,  to  realize  precise  grasping  and manipulation tasks. In this code paper, we propose an online and markerless solution to the eye-hand kinematic calibration of the iCub humanoid robot. We have implemented a sequential Monte Carlo algorithm estimating kinematic calibration parameters (joint offsets) which improve the eye-hand coordination based on the proprioception and vision sensing of the robot. We have shown the usefulness of the developed code and its accuracy on simulation and real-world scenarios. The code is written in C++ and CUDA, where we exploit the GPU to increase the speed of the method. The code is made available online along with a Dataset for testing purposes.</p>
  </span>
  

  
  <span class="bibtex hidden">
    <p>@article{10.3389/frobt.2018.00046,
  author = {Vicente, Pedro and Jamone, Lorenzo and Bernardino, Alexandre},
  title = {Markerless Eye-Hand Kinematic Calibration on the iCub Humanoid Robot},
  journal = {Frontiers in Robotics and AI},
  volume = {5},
  pages = {46},
  year = {2018},
  url = {https://www.frontiersin.org/article/10.3389/frobt.2018.00046},
  doi = {10.3389/frobt.2018.00046},
  issn = {2296-9144},
  pdf = {pvicente-frontiers2018.pdf}
}
</p>
  </span>
  


  
</div>
</li></ol>

<h3 class="year">2016</h3>
<ol class="bibliography"><li>

<div id="vicente2016jint">
  
    <span class="title"><b>Robotic Hand Pose Estimation Based on Stereo Vision and GPU-enabled Internal Graphical Simulation</b></span>
    <span class="author">
      
        
          
            <em>Pedro Vicente</em>,
          
        
      
        
          
            
              Lorenzo Jamone,
            
          
        
      
        
          and
          
            
              Alexandre Bernardino
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>Journal of Intelligent &amp; Robotic Systems</em>
    
    
      2016
    
    </span>
  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
  
  
  [<a href="http://doi.org/10.1007/s10846-016-0376-6" target="_blank">DOI</a>]

  
    [<a href="/assets/pdf/vicente16gpuPoseEst_jint.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
    [<a class="bibtex">Bibtex</a>]
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Humanoid robots have complex kinematic chains whose modeling is error prone. If the robot model is not well calibrated, its hand pose cannot be determined precisely from the encoder readings, and this affects reaching and grasping accuracy. In our work, we propose a novel method to simultaneously i) estimate the pose of the robot hand, and ii) calibrate the robot kinematic model. This is achieved by combining stereo vision, proprioception, and a 3D computer graphics model of the robot. Notably, the use of GPU programming allows to perform the estimation and calibration in real time during the execution of arm reaching movements. Proprioceptive information is exploited to generate hypotheses about the visual appearance of the hand in the camera images, using the 3D computer graphics model of the robot that includes both kinematic and texture information. These hypotheses are compared with the actual visual input using particle filtering, to obtain both i) the best estimate of the hand pose and ii) a set of joint offsets to calibrate the kinematics of the robot model. We evaluate two different approaches to estimate the 6D pose of the hand from vision (silhouette segmentation and edges extraction) and show experimentally that the pose estimation error is considerably reduced with respect to the nominal robot model. Moreover, the GPU implementation ensures a performance about 3 times faster than the CPU one, allowing real-time operation.</p>
  </span>
  

  
  <span class="bibtex hidden">
    <p>@article{vicente2016jint,
  author = {Vicente, Pedro and Jamone, Lorenzo and Bernardino, Alexandre},
  title = {Robotic Hand Pose Estimation Based on Stereo Vision and GPU-enabled Internal Graphical Simulation},
  journal = {Journal of Intelligent {\&amp;} Robotic Systems},
  year = {2016},
  month = sep,
  day = {01},
  volume = {83},
  number = {3},
  pages = {339--358},
  issn = {1573-0409},
  doi = {10.1007/s10846-016-0376-6},
  url = {https://doi.org/10.1007/s10846-016-0376-6},
  pdf = {vicente16gpuPoseEst_jint.pdf}
}
</p>
  </span>
  


  
</div>
</li>
<li>

<div id="vicente2016frontiers">
  
    <span class="title"><b>Online Body Schema Adaptation Based on Internal Mental Simulation and Multisensory Feedback</b></span>
    <span class="author">
      
        
          
            <em>Pedro Vicente</em>,
          
        
      
        
          
            
              Lorenzo Jamone,
            
          
        
      
        
          and
          
            
              Alexandre Bernardino
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>Frontiers in Robotics and AI</em>
    
    
      2016
    
    </span>
  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
  
  
  [<a href="http://doi.org/10.3389/frobt.2016.00007" target="_blank">DOI</a>]

  
    [<a href="/assets/pdf/pvicente-frontiers2016.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
    [<a class="bibtex">Bibtex</a>]
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>In this paper, we describe a novel approach to obtain automatic adaptation of the robot body schema and to improve the robot perceptual and motor skills based on this body knowledge. Predictions obtained through a mental simulation of the body are combined with the real sensory feedback to achieve two objectives simultaneously: body schema adaptation and markerless 6D hand pose estimation. The body schema consists of a computer graphics simulation of the robot, which includes the arm and head kinematics (adapted online during the movements) and an appearance model of the hand shape and texture. The mental simulation process generates predictions on how the hand will appear in the robot camera images, based on the body schema and the proprioceptive information (i.e. motor encoders). These predictions are compared to the actual images using Sequential Monte Carlo techniques to feed a particle-based Bayesian estimation method to estimate the parameters of the body schema. The updated body schema will improve the estimates of the 6D hand pose, which is then
    used in a closed-loop control scheme (i.e. visual servoing), enabling precise reaching. We report experiments with the iCub humanoid robot that support the validity of our approach. A number of simulations with precise ground-truth were performed to evaluate the estimation capabilities of the proposed framework. Then, we show how the use of high-performance GPU programming and an edge-based algorithm for visual perception allow for real-time implementation in real world scenarios.</p>
  </span>
  

  
  <span class="bibtex hidden">
    <p>@article{vicente2016frontiers,
  author = {Vicente, Pedro and Jamone, Lorenzo and Bernardino, Alexandre},
  title = {Online Body Schema Adaptation Based on Internal Mental Simulation and Multisensory Feedback},
  journal = {Frontiers in Robotics and AI},
  volume = {3},
  pages = {7},
  year = {2016},
  url = {https://www.frontiersin.org/article/10.3389/frobt.2016.00007},
  doi = {10.3389/frobt.2016.00007},
  issn = {2296-9144},
  pdf = {pvicente-frontiers2016.pdf}
}
</p>
  </span>
  


  
</div>
</li></ol>

<h2 class="year">Conferences</h2>

<h3 class="year">2020</h3>
<ol class="bibliography"><li>

<div id="gcunha2020recpad">
  
    <span class="title"><b>Active Robot Learning for Efficient Body-Schema Online Adaptation</b></span>
    <span class="author">
      
        
          
            
              Gonçalo Cunha,
            
          
        
      
        
          
            
              Alexandre Bernardino,
            
          
        
      
        
          
            <em>Pedro Vicente</em>,
          
        
      
        
          
            
              Ricardo Ribeiro,
            
          
        
      
        
          and
          
            
              Plínio Moreno
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Portuguese Conference on Pattern Recognition (RecPad)</em>
    
    
      2020
    
    </span>
  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
  
  
  
    [<a href="/assets/pdf/acunha-recpad2020.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
    [<a class="bibtex">Bibtex</a>]
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>
    This work proposes an active learning approach for estimating the Denavit-Hartenberg parameters of 7 joints of the iCub arm in a simulation 
    environment, using observations of the end-effector’s pose and knowing the values from proprioceptive sensors. Cost-sensitive active learning, 
    aims to reduce the number of measurements taken and also reduce the total movement performed by the robot while calibrating, thus reducing 
    energy consumption, along with mechanical fatigue and wear. The estimation of the arm’s parameters is done using the Extended Kalman Filter 
    and the active exploration is guided by the A-Optimality criterion. The results show cost-sensitive active learning can perform similarly to 
    the straightforward active learning approach, while reducing significantly the necessary movement.
  </p>
  </span>
  

  
  <span class="bibtex hidden">
    <p>@inproceedings{gcunha2020recpad,
  title = {{Active Robot Learning for Efficient Body-Schema Online Adaptation}},
  author = {Cunha, Gonçalo and Bernardino, Alexandre and Vicente, Pedro and and Ribeiro, Ricardo and Moreno, Plínio},
  booktitle = {Portuguese Conference on Pattern Recognition (RecPad)},
  year = {2020},
  pdf = {acunha-recpad2020.pdf},
  bestp = {Best Poster Award}
}
</p>
  </span>
  


  
  <span class="Best Paper Award">
    <p style="color:red"> <b>[Best Poster Award]</b></p>
  </span>
  
</div>
</li>
<li>

<div id="mnascimento2020icarsc">
  
    <span class="title"><b>2D Visual Servoing meets Rapidly-exploring Random Trees for collision avoidance</b></span>
    <span class="author">
      
        
          
            
              Miguel Nascimento,
            
          
        
      
        
          
            <em>Pedro Vicente</em>,
          
        
      
        
          and
          
            
              Alexandre Bernardino
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In IEEE International Conference on Autonomous Robot Systems and Competitions (ICARSC)</em>
    
    
      2020
    
    </span>
  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
  
  
  [<a href="http://doi.org/10.1109/ICARSC49921.2020.9096133" target="_blank">DOI</a>]

  
    [<a href="/assets/pdf/mnascimento2020icarsc.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
    [<a class="bibtex">Bibtex</a>]
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>
    Visual Servoing is a well-known subject in robotics. However, there are still some challenges on the visual control of robots for applications 
    in human environments. In this article, we propose a method for path planning and correction of kinematic errors using visual servoing. 
    3D information provided by external cameras will be used for segmenting the environment and detecting the obstacles in the scene. 
    Rapidly-exploring Random Trees are then used to calculate a path through the obstacles to a given, previously calculated, end-effector goal pose. 
    This allows for model-free path planning for cluttered environments by using a point cloud representation of the environment. 
    The proposed path is then followed by the robot in open-loop. Error correction is performed near the goal pose by using real-time calculated 
    image features as control points for an Image-Based Visual Servoing controller that drives the end-effector towards the desired goal pose. 
    With this method, we intend to achieve the navigation of a robotic arm through a cluttered environment towards a goal pose with error correction 
    performed at the end of the trajectory to mitigate both the weaknesses of Image Based Visual Servoing and of open-loop trajectory following. 
    We made several experiments in order to validate our approach by evaluating each individual main component (environment segmentation, trajectory 
    calculation and error correction through visual servoing) of our solution. Furthermore, our solution was implemented in ROS using the Baxter 
    Research Robot.
  </p>
  </span>
  

  
  <span class="bibtex hidden">
    <p>@inproceedings{mnascimento2020icarsc,
  author = {{Nascimento}, Miguel and {Vicente}, Pedro and {Bernardino}, Alexandre},
  booktitle = {IEEE International Conference on Autonomous Robot Systems and Competitions (ICARSC)},
  title = {2D Visual Servoing meets Rapidly-exploring Random Trees for collision avoidance},
  year = {2020},
  pages = {227-232},
  doi = {10.1109/ICARSC49921.2020.9096133},
  bestp = {Highly Commended Paper Award},
  pdf = {mnascimento2020icarsc.pdf}
}
</p>
  </span>
  


  
  <span class="Best Paper Award">
    <p style="color:red"> <b>[Highly Commended Paper Award]</b></p>
  </span>
  
</div>
</li></ol>

<h3 class="year">2019</h3>
<ol class="bibliography"><li>

<div id="adehban2019icdl">
  
    <span class="title"><b>Robotic Interactive Physics Parameters Estimator (RIPPE)</b></span>
    <span class="author">
      
        
          
            
              Atabak Dehban,
            
          
        
      
        
          
            
              Carlos Cardoso,
            
          
        
      
        
          
            <em>Pedro Vicente</em>,
          
        
      
        
          
            
              Alexandre Bernardino,
            
          
        
      
        
          and
          
            
              José Santos-Victor
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Joint IEEE International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob)</em>
    
    
      2019
    
    </span>
  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
  
  
  
    [<a href="/assets/pdf/adehban-icdl2019.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
    [<a class="bibtex">Bibtex</a>]
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>
    The ability to reason about natural laws of an environment directly contributes to successful performance in it.
    In this work, we present RIPPE, a framework that allows a robot to leverage existing physics simulators as its 
    knowledge base for learning interactions with in-animate objects. To achieve this, the robot needs to initially
    interact with its surrounding environment and observe the effects of its behaviours. Relying on the simulator
    to efficiently solve the partial differential equations describing these physical interactions, the robot 
    infers consistent physical parameters of its surroundings by repeating the same actions in simulation and 
    evaluate how closely they match its real observations. The learning process is performed using Bayesian 
    Optimisation techniques to sample efficiently the parameter space. We assess the utility of these inferred
    parameters by measuring how well they can explain physical interactions using previously unseen actions and tools.
  </p>
  </span>
  

  
  <span class="bibtex hidden">
    <p>@inproceedings{adehban2019icdl,
  title = {{Robotic Interactive Physics Parameters Estimator (RIPPE)}},
  author = {Dehban, Atabak and Cardoso, Carlos and Vicente, Pedro and Bernardino, Alexandre and Santos-Victor, José},
  booktitle = {Joint IEEE International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob)},
  year = {2019},
  organization = {IEEE},
  pdf = {adehban-icdl2019.pdf}
}
</p>
  </span>
  


  
</div>
</li></ol>

<h3 class="year">2018</h3>
<ol class="bibliography"><li>

<div id="jcastanheira2018iros">
  
    <span class="title"><b>Finding safe 3D robot grasps through efficient haptic exploration with unscented Bayesian optimization and collision penalty</b></span>
    <span class="author">
      
        
          
            
              João Castanheira,
            
          
        
      
        
          
            <em>Pedro Vicente</em>,
          
        
      
        
          
            
              Ruben Martinez-Cantin,
            
          
        
      
        
          
            
              Lorenzo Jamone,
            
          
        
      
        
          and
          
            
              Alexandre Bernardino
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>
    
    
      2018
    
    </span>
  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
  
  
  
    [<a href="/assets/pdf/jcastanheira-iros2018.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
    [<a class="bibtex">Bibtex</a>]
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>
    Robust grasping is a major, and still unsolved,
    problem in robotics. Information about the 3D shape of an
    object can be obtained either from prior knowledge (e.g.,
    accurate models of known objects or approximate models of
    familiar objects) or real-time sensing (e.g., partial point clouds
    of unknown objects) and can be used to identify good potential
    grasps. However, due to modeling and sensing inaccuracies,
    local exploration is often needed to refine such grasps and
    successfully apply them in the real world. The recently proposed
    unscented Bayesian optimization technique can make such exploration safer by selecting grasps that are robust to uncertainty
    in the input space (e.g., inaccuracies in the grasp execution).
    Extending our previous work on 2D optimization, in this paper
    we propose a 3D haptic exploration strategy that combines
    unscented Bayesian optimization with a novel collision penalty
    heuristic to find safe grasps in a very efficient way: while by
    augmenting the search-space to 3D we are able to find better
    grasps, the collision penalty heuristic allows us to do so without
    increasing the number of exploration steps.
  </p>
  </span>
  

  
  <span class="bibtex hidden">
    <p>@inproceedings{jcastanheira2018iros,
  title = {{Finding safe 3D robot grasps through efficient haptic exploration with unscented Bayesian optimization and collision penalty}},
  author = {Castanheira, João and Vicente, Pedro and Martinez-Cantin, Ruben and Jamone, Lorenzo and Bernardino, Alexandre},
  booktitle = {IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  year = {2018},
  organization = {IEEE},
  pdf = {jcastanheira-iros2018.pdf}
}
</p>
  </span>
  


  
</div>
</li>
<li>

<div id="rzenha2018icdl">
  
    <span class="title"><b>Incremental adaptation of a robot body schema based on touch events</b></span>
    <span class="author">
      
        
          
            
              Rodrigo Zenha,
            
          
        
      
        
          
            <em>Pedro Vicente</em>,
          
        
      
        
          
            
              Lorenzo Jamone,
            
          
        
      
        
          and
          
            
              Alexandre Bernardino
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Joint IEEE International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob)</em>
    
    
      2018
    
    </span>
  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
  
  
  
    [<a href="/assets/pdf/rzenha_icdl2018.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
    [<a class="bibtex">Bibtex</a>]
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>
    The term ‘body schema’ refers to a computational
    representation of a physical body; the neural representation of
    a human body, or the numerical representation of a robot body.
    In both humans and robots, such a representation is crucial to
    accurately control body movements. While humans learn and
    continuously adapt their body schema based on multimodal
    perception and neural plasticity, robots are typically assigned
    with a fixed analytical model (e.g., the robot kinematics) which
    describes their bodies. However, there are always discrepancies
    between a model and the real robot, and they vary over
    time, thus affecting the accuracy of movement control. In
    this work, we equip a humanoid robot with the ability to
    incrementally estimate such model inaccuracies by touching
    known planar surfaces (e.g., walls) in its vicinity through
    motor babbling exploration, effectively adapting its own body
    schema based on the contact information alone. The problem
    is formulated as an adaptive parameter estimation (Extended
    Kalman Filter) which makes use of planar constraints obtained
    at each contact detection. We compare different incremental
    update methods through an extensive set of experiments with a
    realistic simulation of the iCub humanoid robot, showing that
    the model inaccuracies can be reduced by more than 80%.
  </p>
  </span>
  

  
  <span class="bibtex hidden">
    <p>@inproceedings{rzenha2018icdl,
  title = {{Incremental adaptation of a robot body schema based on touch events}},
  author = {Zenha, Rodrigo and Vicente, Pedro and Jamone, Lorenzo and Bernardino, Alexandre},
  booktitle = {Joint IEEE International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob)},
  year = {2018},
  organization = {IEEE},
  pdf = {rzenha_icdl2018.pdf}
}
</p>
  </span>
  


  
</div>
</li>
<li>

<div id="cauli2018autonomous">
  
    <span class="title"><b>Autonomous table-cleaning from kinesthetic demonstrations using Deep Learning</b></span>
    <span class="author">
      
        
          
            
              Nino Cauli,
            
          
        
      
        
          
            <em>Pedro Vicente</em>,
          
        
      
        
          
            
              Jaeseok Kim,
            
          
        
      
        
          
            
              Bruno Damas,
            
          
        
      
        
          
            
              Alexandre Bernardino,
            
          
        
      
        
          
            
              Filippo Cavallo,
            
          
        
      
        
          and
          
            
              José Santos-Victor
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Joint IEEE International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob)</em>
    
    
      2018
    
    </span>
  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
  
  
  
    [<a href="/assets/pdf/ncauli_icdl2018.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
    [<a class="bibtex">Bibtex</a>]
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>
    We address the problem of teaching a robot how
    to autonomously perform table-cleaning tasks in a robust way.
    In particular, we focus on wiping and sweeping a table with
    a tool (e.g., a sponge). For the training phase, we use a set
    of kinestethic demonstrations performed over a table. The
    recorded 2D table-space trajectories, together with the images
    acquired by the robot, are used to train a deep convolutional
    network that automatically learns the parameters of a Gaussian
    Mixture Model that represents the hand movement. After the
    learning stage, the network is fed with the current image show-
    ing the location/shape of the dirt or stain to clean. The robot
    is able to perform cleaning arm-movements, obtained through
    Gaussian Mixture Regression using the mixture parameters
    provided by the network. Invariance to the robot posture is
    achieved by applying a plane-projective transformation before
    inputting the images to the neural network; robustness to
    illumination changes and other disturbances is increased by
    considering an augmented data set. This improves the general-
    ization properties of the neural network, enabling for instance
    its use with the left arm after being trained using trajectories
    acquired with the right arm. The system was tested on the
    iCub robot generating a cleaning behaviour similar to the one
    of human demonstrators.
  </p>
  </span>
  

  
  <span class="bibtex hidden">
    <p>@inproceedings{cauli2018autonomous,
  title = {{Autonomous table-cleaning from kinesthetic demonstrations using Deep Learning}},
  author = {Cauli, Nino and Vicente, Pedro and Kim, Jaeseok and Damas, Bruno and Bernardino, Alexandre and Cavallo, Filippo and Santos-Victor, Jos{\'e}},
  booktitle = {Joint IEEE International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob)},
  year = {2018},
  organization = {IEEE},
  pdf = {ncauli_icdl2018.pdf}
}
</p>
  </span>
  


  
</div>
</li>
<li>

<div id="kim2018icub">
  
    <span class="title"><b>“iCub, clean the table!” A robot learning from demonstration approach using deep neural networks</b></span>
    <span class="author">
      
        
          
            
              Jaeseok Kim,
            
          
        
      
        
          
            
              Nino Cauli,
            
          
        
      
        
          
            <em>Pedro Vicente</em>,
          
        
      
        
          
            
              Bruno Damas,
            
          
        
      
        
          
            
              Filippo Cavallo,
            
          
        
      
        
          and
          
            
              José Santos-Victor
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In IEEE International Conference on Autonomous Robot Systems and Competitions (ICARSC)</em>
    
    
      2018
    
    </span>
  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
  
  
  [<a href="http://doi.org/10.1109/ICARSC.2018.8374152" target="_blank">DOI</a>]

  
    [<a href="/assets/pdf/jkim-icarsc2018.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
    [<a class="bibtex">Bibtex</a>]
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>
    Autonomous service robots have become a key
    research topic in robotics, particularly for household chores.
    A typical home scenario is highly unconstrained and a service
    robot needs to adapt constantly to new situations. In this
    paper, we address the problem of autonomous cleaning tasks
    in uncontrolled environments. In our approach, a human
    instructor uses kinestethic demonstrations to teach a robot how
    to perform different cleaning tasks on a table. Then, we use Task
    Parametrized Gaussian Mixture Models (TP-GMMs) to encode
    the demonstrations variability, while providing appropriate
    generalization abilities. TP-GMMs extend Gaussian Mixture
    Models with an auxiliary set of reference frames, in order to ex-
    trapolate the demonstrations to different task parameters such
    as movement locations, amplitude or orientations. However, the
    reference frames (that parametrize TP-GMMs) can be very
    difficult to extract in practice, as it may require segmenting the
    cluttered images of the working table-top. Instead, in this work
    the reference frames are automatically extracted from robot
    camera images, using a deep neural network that was trained
    during human demonstrations of a cleaning task. This approach
    has two main benefits: (i) it takes the human completely out of
    the loop while performing complex cleaning tasks; and (ii) the
    network is able to identify the specific task to be performed
    directly from image data, thus also enabling automatic task
    selection from a set of previously demonstrated tasks. The
    system was implemented on the iCub humanoid robot. During
    the tests, the robot was able to successfully clean a table with
    two different types of dirt (wiping a marker’s scribble or
    sweeping clusters of lentils).
  </p>
  </span>
  

  
  <span class="bibtex hidden">
    <p>@inproceedings{kim2018icub,
  title = {{``iCub, clean the table!'' A robot learning from demonstration approach using deep neural networks}},
  author = {Kim, Jaeseok and Cauli, Nino and Vicente, Pedro and Damas, Bruno and Cavallo, Filippo and Santos-Victor, Jos{\'e}},
  booktitle = {IEEE International Conference on Autonomous Robot Systems and Competitions (ICARSC)},
  pages = {3--9},
  year = {2018},
  organization = {IEEE},
  pdf = {jkim-icarsc2018.pdf},
  bestp = {Best Conference Paper Award},
  doi = {10.1109/ICARSC.2018.8374152}
}
</p>
  </span>
  


  
  <span class="Best Paper Award">
    <p style="color:red"> <b>[Best Conference Paper Award]</b></p>
  </span>
  
</div>
</li></ol>

<h3 class="year">2017</h3>
<ol class="bibliography"><li>

<div id="saponaro2017icdl">
  
    <span class="title"><b>Learning at the ends: From hand to tool affordances in humanoid robots</b></span>
    <span class="author">
      
        
          
            
              G. Saponaro,
            
          
        
      
        
          
            <em>P. Vicente</em>,
          
        
      
        
          
            
              A. Dehban,
            
          
        
      
        
          
            
              L. Jamone,
            
          
        
      
        
          
            
              A. Bernardino,
            
          
        
      
        
          and
          
            
              J. Santos-Victor
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Joint IEEE International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob)</em>
    
    
      2017
    
    </span>
  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
  
  
  [<a href="http://doi.org/10.1109/DEVLRN.2017.8329826" target="_blank">DOI</a>]

  
    [<a href="/assets/pdf/gsaponaro-icdlepirob2017.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
    [<a class="bibtex">Bibtex</a>]
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>
    One of the open challenges in designing robots that
    operate successfully in the unpredictable human environment
    is how to make them able to predict what actions they can
    perform on objects, and what their effects will be, i.e., the ability
    to perceive object affordances. Since modeling all the possible
    world interactions is unfeasible, learning from experience is
    required, posing the challenge of collecting a large amount
    of experiences (i.e., training data). Typically, a manipulative
    robot operates on external objects by using its own hands (or
    similar end-effectors), but in some cases the use of tools may be
    desirable; nevertheless, it is reasonable to assume that while a
    robot can collect many sensorimotor experiences using its own
    hands, this cannot happen for all possible human-made tools.
    Therefore, in this paper we investigate the developmental
    transition from hand to tool affordances: what sensorimotor
    skills that a robot has acquired with its bare hands can be
    employed for tool use? By employing a visual and motor
    imagination mechanism to represent different hand postures
    compactly, we propose a probabilistic model to learn hand
    affordances, and we show how this model can generalize to
    estimate the affordances of previously unseen tools, ultimately
    supporting planning, decision-making and tool selection tasks
    in humanoid robots. We present experimental results with the
    iCub humanoid robot, and we publicly release the collected
    sensorimotor data in the form of a hand posture affordances
    dataset.
  </p>
  </span>
  

  
  <span class="bibtex hidden">
    <p>@inproceedings{saponaro2017icdl,
  author = {Saponaro, G. and Vicente, P. and Dehban, A. and Jamone, L. and Bernardino, A. and Santos-Victor, J.},
  booktitle = {Joint IEEE International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob)},
  title = {{Learning at the ends: From hand to tool affordances in humanoid robots}},
  year = {2017},
  volume = {},
  number = {},
  pages = {331-337},
  keywords = {end effectors;humanoid robots;human-robot interaction;learning (artificial intelligence);learning systems;tool affordances;humanoid robots;open challenges;unpredictable human environment;object affordances;possible world interactions;training data;manipulative robot;external objects;similar end-effectors;sensorimotor experiences;sensorimotor skills;bare hands;visual motor imagination mechanism;different hand postures;probabilistic model;unseen tools;tool selection tasks;iCub humanoid robot;collected sensorimotor data;hand posture affordances dataset;possible human-made tools;Tools;Robot sensing systems;Visualization;Humanoid robots;Solid modeling;Shape},
  doi = {10.1109/DEVLRN.2017.8329826},
  issn = {2161-9484},
  pdf = {gsaponaro-icdlepirob2017.pdf},
  month = sep
}
</p>
  </span>
  


  
</div>
</li>
<li>

<div id="vicente2017icra">
  
    <span class="title"><b>Towards markerless visual servoing of grasping tasks for humanoid robots</b></span>
    <span class="author">
      
        
          
            <em>P. Vicente</em>,
          
        
      
        
          
            
              L. Jamone,
            
          
        
      
        
          and
          
            
              A. Bernardino
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In IEEE International Conference on Robotics and Automation (ICRA)</em>
    
    
      2017
    
    </span>
  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
  
  
  [<a href="http://doi.org/10.1109/ICRA.2017.7989441" target="_blank">DOI</a>]

  
    [<a href="/assets/pdf/pvicente_ICRA2017.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
    [<a class="bibtex">Bibtex</a>]
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>
    Vision-based grasping for humanoid robots is a
    challenging problem due to a multitude of factors. First,
    humanoid robots use an “eye-to-hand” kinematics configuration
    that, on the contrary to the more common “eye-in-hand”
    configuration, demands a precise estimate of the position of the
    robot’s hand. Second, humanoid robots have a long kinematic
    chain from the eyes to the hands, prone to accumulate the
    calibration errors of the kinematics model, which offsets the
    measured hand-to-object relative pose from the real one. In
    this paper, we propose a method able to solve these two
    issues jointly. A robust pose estimation of the robot’s hand
    is achieved via a 3D model-based stereo-vision algorithm, using
    an edge-based distance transform metric and synthetically
    generated images of a robot’s arm-hand internal computer-
    graphics model (kinematics and appearance). Then, a particle-
    based optimisation method adapts on-line the robot’s internal
    model to match the real and the synthetically generated images,
    effectively compensating the kinematics calibration errors. We
    evaluate the proposed approach using a position-based visual-
    servoing method on the iCub robot, showing the importance of
    the continuous visual feedback in humanoid grasping tasks.
  </p>
  </span>
  

  
  <span class="bibtex hidden">
    <p>@inproceedings{vicente2017icra,
  author = {Vicente, P. and Jamone, L. and Bernardino, A.},
  booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
  title = {{Towards markerless visual servoing of grasping tasks for humanoid robots}},
  year = {2017},
  volume = {},
  number = {},
  pages = {3811-3816},
  keywords = {computer graphics;error analysis;humanoid robots;manipulator kinematics;optimisation;pose estimation;robot vision;stereo image processing;3D model-based stereo-vision algorithm;edge-based distance transform metric;eye-to-hand kinematics configuration;grasping task visual servoing;hand-to-object relative pose measurement;humanoid grasping tasks;iCub robot;kinematic calibration errors;position estimation;robot arm-hand internal computer-graphic model;robust robot pose estimation;synthetically generated images;vision-based grasping;visual feedback;Calibration;Grasping;Humanoid robots;Kinematics;Solid modeling;Visualization},
  doi = {10.1109/ICRA.2017.7989441},
  issn = {},
  pdf = {pvicente_ICRA2017.pdf},
  month = may
}
</p>
  </span>
  


  
</div>
</li>
<li>

<div id="vicente2017icarsc">
  
    <span class="title"><b>Wedding robotics: A case study</b></span>
    <span class="author">
      
        
          
            <em>P. Vicente</em>,
          
        
      
        
          and
          
            
              A. Bernardino
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In IEEE International Conference on Autonomous Robot Systems and Competitions (ICARSC)</em>
    
    
      2017
    
    </span>
  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
  
  
  [<a href="http://doi.org/10.1109/ICARSC.2017.7964066" target="_blank">DOI</a>]

  
    [<a href="/assets/pdf/pvicente-icarsc2017.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
    [<a class="bibtex">Bibtex</a>]
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>
    In this work, we propose to study a social robot in a
    wedding context, where it plays the role of a wedding ring bearer.
    We focus on the interaction with the audience, their expectations,
    and reactions, rather than in technical details. We collect data
    from 121 individuals belonging to two different groups, those
    who have seen the robot behaviour (live or recorded versions) and
    those who did not see the robot performance. We divide the study
    into three parts: i) the reactions of the guests at the wedding,
    ii) a comparison between subjects which were exposed or not to
    the robot behaviour, and iii) a within-subjects experiment where
    after filling a survey, they are asked to see the recorded robot
    behaviour. The guests reacted positively to the experiment. The
    robot was considered likeable, lively and safe by the majority
    of the participants in the study. The group that observed the
    robot’s behaviour had a better opinion on the use of robots in
    wedding ceremonies than the group that did not observe the
    experience. This may suggest that a higher presence of robots in
    social activities will increase the acceptance of robots in society.
    Index Terms—humanoid robot, social robotics, human-robot
    interaction, social experiment, case study.
  </p>
  </span>
  

  
  <span class="bibtex hidden">
    <p>@inproceedings{vicente2017icarsc,
  author = {Vicente, P. and Bernardino, A.},
  booktitle = {IEEE International Conference on Autonomous Robot Systems and Competitions (ICARSC)},
  title = {{Wedding robotics: A case study}},
  year = {2017},
  volume = {},
  number = {},
  pages = {140-145},
  keywords = {mobile robots;robot behaviour;robot performance;social robot;technical details;wedding guest reactions;wedding ring bearer;wedding robotics;Conferences;Context;Humanoid robots;Robot sensing systems;Service robots;Software;case study;human-robot interaction;humanoid robot;social experiment;social robotics},
  doi = {10.1109/ICARSC.2017.7964066},
  pdf = {pvicente-icarsc2017.pdf},
  month = apr
}
</p>
  </span>
  


  
</div>
</li></ol>

<h3 class="year">2015</h3>
<ol class="bibliography"><li>

<div id="vicente2015icarsc">
  
    <span class="title"><b>GPU-Enabled Particle Based Optimization for Robotic-Hand Pose Estimation and Self-Calibration</b></span>
    <span class="author">
      
        
          
            <em>Pedro Vicente</em>,
          
        
      
        
          
            
              Ricardo Ferreira,
            
          
        
      
        
          
            
              Lorenzo Jamone,
            
          
        
      
        
          and
          
            
              Alexandre Bernardino
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In IEEE International Conference on Autonomous Robot Systems and Competitions (ICARSC)</em>
    
    
      2015
    
    </span>
  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
  
  
  [<a href="http://doi.org/10.1109/ICARSC.2015.25" target="_blank">DOI</a>]

  
    [<a href="/assets/pdf/pvicente-ICARSC15.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
    [<a class="bibtex">Bibtex</a>]
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>
    Humanoid robots have complex kinematic chains
    that are difficult to model with the precision required to reach
    and/or grasp objects properly. In this paper we propose a GPU-
    enabled vision based 3D hand pose estimation method that
    runs during robotic reaching tasks to calibrate in real time the
    kinematic chain of the robot arm. This is achieved by combining:
    i) proprioceptive and visual sensing; and ii) a kinematic and
    computer graphics model of the system. We use proprioceptive
    input to create visual hypotheses about the hand appearance in
    the image using a 3D CAD model inside the game engine from
    Unity Technologies. These hypotheses are compared with the
    actual visual input using particle filter techniques. The outcome
    of this processing is the best hypothesis for the hand pose and a
    set of joint offsets to calibrate the arm. We tested our approach
    in a simulation environment and verified that the angular error is
    reduced 3 times and the position error about 12 times comparing
    with the non-calibrated case (proprioception only). The used
    GPU implementation techniques ensures a performance 2.5 times
    faster than performing the computations on the CPU.
    Index Terms—humanoid robot, robotic-hand pose estimation,
    robot self-calibration, 3D model based tracking, GPU, reaching.
  </p>
  </span>
  

  
  <span class="bibtex hidden">
    <p>@inproceedings{vicente2015icarsc,
  author = {Vicente, Pedro and Ferreira, Ricardo and Jamone, Lorenzo and Bernardino, Alexandre},
  booktitle = {IEEE International Conference on Autonomous Robot Systems and Competitions (ICARSC)},
  title = {{GPU-Enabled Particle Based Optimization for Robotic-Hand Pose Estimation and Self-Calibration}},
  year = {2015},
  pages = {3-8},
  keywords = {CAD;control engineering computing;graphics processing units;humanoid robots;manipulator kinematics;optimisation;particle filtering (numerical methods);pose estimation;robot vision;solid modelling;3D CAD model;3D hand pose estimation method;GPU-enabled particle based optimization;GPU-enabled vision;Unity Technologies;computer graphics model;game engine;humanoid robots;kinematic chains;particle filter techniques;proprioceptive sensing;robot arm;robotic-hand pose estimation;robotic-hand self-calibration;visual hypotheses;visual sensing;Computational modeling;Estimation;Graphics processing units;Joints;Kinematics;Robots;Visualization;3D model based tracking;GPU;humanoid robot;reaching;robot self-calibration;robotic-hand pose estimation},
  doi = {10.1109/ICARSC.2015.25},
  pdf = {pvicente-ICARSC15.pdf},
  month = apr
}
</p>
  </span>
  


  
</div>
</li></ol>

<h3 class="year">2014</h3>
<ol class="bibliography"><li>

<div id="vicente2014icdl">
  
    <span class="title"><b>Eye-hand online adaptation during reaching tasks in a humanoid robot</b></span>
    <span class="author">
      
        
          
            <em>Pedro Vicente</em>,
          
        
      
        
          
            
              Ricardo Ferreira,
            
          
        
      
        
          
            
              Lorenzo Jamone,
            
          
        
      
        
          and
          
            
              Alexandre Bernardino
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Joint IEEE International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob)</em>
    
    
      2014
    
    </span>
  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
  
  
  [<a href="http://doi.org/10.1109/DEVLRN.2014.6982978" target="_blank">DOI</a>]

  
    [<a href="/assets/pdf/vicente14icdlEyeHand.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
    [<a class="bibtex">Bibtex</a>]
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>
    In this paper we propose a method for the online
    adaptation of a humanoid robot’s arm kinematics, using its
    visual and proprioceptive sensors. A typical reaching movement
    starts with a ballistic open-loop phase to bring the hand to the
    vicinity of the object. During this phase, as soon as the hand of
    the robot enters the field of view of one of its cameras, a vision
    based 3D hand pose estimation method feeds a particle filter that
    gradually adjusts the arm kinematics’ parameters. Our method
    makes use of a 3D CAD model of the robot hand (geometry
    and texture) whose predicted position in the image is compared
    at each time step with the cameras’ incoming information.
    When the hand gets close to the object, the kinematic errors
    have reduced significantly and a better control of grasping can
    eventually be achieved. We have tested the method both in
    simulation and with the real robot and verify error decreases
    by a factor of 3 during a typical reaching time span.
    Index Terms—Online adaptation, internal model learning, 3D
    model based tracking, reaching, humanoid robot.
  </p>
  </span>
  

  
  <span class="bibtex hidden">
    <p>@inproceedings{vicente2014icdl,
  author = {Vicente, Pedro and Ferreira, Ricardo and Jamone, Lorenzo and Bernardino, Alexandre},
  booktitle = {Joint IEEE International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob)},
  title = {{Eye-hand online adaptation during reaching tasks in a humanoid robot}},
  year = {2014},
  pages = {175-180},
  organization = {IEEE},
  keywords = {CAD;cameras;dexterous manipulators;humanoid robots;image sensors;manipulator kinematics;particle filtering (numerical methods);pose estimation;robot vision;solid modelling;ballistic open-loop phase;cameras;eye-hand online adaptation;humanoid robot arm kinematics;kinematic errors;particle filter;proprioceptive sensors;reaching tasks;robot hand 3D CAD model;vision based 3D hand pose estimation method;visual sensors;Cameras;Joints;Kinematics;Robot vision systems;Visualization;3D model based tracking;Online adaptation;humanoid robot;internal model learning;reaching},
  doi = {10.1109/DEVLRN.2014.6982978},
  pdf = {vicente14icdlEyeHand.pdf},
  month = oct
}
</p>
  </span>
  


  
</div>
</li></ol>

<h2 class="year">Under review</h2>

<h3 class="year">2021</h3>
<ol class="bibliography"><li>

<div id="amenezes2021icra">
  
    <span class="title"><b>From rocks to walls: a model-free reinforcement learning approach to dry stacking with irregular rocks</b></span>
    <span class="author">
      
        
          
            
              André Menezes,
            
          
        
      
        
          
            <em>Pedro Vicente</em>,
          
        
      
        
          
            
              Alexandre Bernardino,
            
          
        
      
        
          and
          
            
              Rodrigo Ventura
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In IEEE International Conference on Robotics and Automation (ICRA)</em>
    
    
      2021
    
    </span>
  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
  
  
  
  
  
  
  
  
    [<a class="bibtex">Bibtex</a>]
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>
  In-situ  resource  utilization (ISRU) is a key aspect for an efficient human exploration of extraterrestrial environments. A cost-effective method for the construction of preliminary structures is dry stacking with locally found unprocessed rocks, which is a challenging task. This work focus on learning this task from scratch.
  Former approaches rely on previously acquired models, which may be hard to obtain in the context of a mission. In alternative, we propose a model-free, data driven approach. 
  We formulate an abstraction of the problem as the task of selecting the position to place each rock, presented to the robot in a sequence, on top of the currently built structure. The goal is to assemble a wall that approximates a target volume, given the 3D perception of the currently built structure, the next object and the target volume. An agent is developed to learn this task using reinforcement learning. The deep Q-networks (DQN) algorithm is used, where the Q-network outputs a value map corresponding to the expected return of placing the object in each position of a top-view depth image. 
  The learned q-function is able to capture the goal and dynamics of the environment. The emerged behaviour is, to some extent, consistent with dry stacking theory. 
  The learned policy outperforms engineered heuristics, both in terms of stability of the structure and similarity with the target volume.
  Despite the simplification of the task, the policy learned with this approach could be applied to a realistic setting as the high level planner in an autonomous construction pipeline.
  </p>
  </span>
  

  
  <span class="bibtex hidden">
    <p>@inproceedings{amenezes2021icra,
  title = {{From rocks to walls: a model-free reinforcement learning approach to dry stacking with irregular rocks}},
  author = {Menezes, André and Vicente, Pedro and Bernardino, Alexandre and Ventura, Rodrigo},
  booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
  year = {2021}
}
</p>
  </span>
  


  
</div>
</li></ol>


  </article>

  

  
    <div class="social">
  <span class="contacticon center">
    <a href="mailto:%70%76%69%63%65%6E%74%65@%69%73%72.%74%65%63%6E%69%63%6F.%75%6C%69%73%62%6F%61.%70%74"><i class="fas fa-envelope"></i></a>
    <a href="https://orcid.org/0000-0002-9678-9055" target="_blank" title="ORCID"><i class="ai ai-orcid"></i></a>
    <a href="https://scholar.google.com/citations?user=esv9CkAAAAAJ" target="_blank" title="Google Scholar"><i class="ai ai-google-scholar"></i></a>
    <a href="https://github.com/vicentepedro" target="_blank" title="GitHub"><i class="fab fa-github"></i></a>
    <a href="https://www.linkedin.com/in/pedro-vicente-b95b623b" target="_blank" title="LinkedIn"><i class="fab fa-linkedin"></i></a>
    
    
    <a href="https://www.researchgate.net/profile/Pedro_Vicente6" target="_blank" title="Research Gate"><i class="ai ai-researchgate"></i></a>
    
  </span>

  <div class="col three caption">
    For an updated list of my publications please refer to google scholar account.

  </div>
</div>

  

</div>

      </div>
    </div>

    <footer>

  <div class="wrapper">
    &copy; Copyright 2020 Pedro Vicente.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
  </div>

</footer>


    <!-- Load jQuery -->
<script src="//code.jquery.com/jquery-1.12.4.min.js"></script>

<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


<!-- Load KaTeX -->
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.js"></script>
<script src="/assets/js/katex.js"></script>




<!-- Include custom icon fonts -->
<link rel="stylesheet" href="/assets/css/fontawesome-all.min.css">
<link rel="stylesheet" href="/assets/css/academicons.min.css">

<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-138589028-1', 'auto');
ga('send', 'pageview');
</script>


  </body>

</html>
