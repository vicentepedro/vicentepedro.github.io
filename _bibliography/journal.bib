@Article{vicente2016jint,
    author="Vicente, Pedro and Jamone, Lorenzo and Bernardino, Alexandre",
    title="Robotic Hand Pose Estimation Based on Stereo Vision and GPU-enabled Internal Graphical Simulation",
    journal="Journal of Intelligent {\&} Robotic Systems",
    year="2016",
    month="Sep",
    day="01",
    volume="83",
    number="3",
    pages="339--358",
    abstract="Humanoid robots have complex kinematic chains whose modeling is error prone. If the robot model is not well calibrated, its hand pose cannot be determined precisely from the encoder readings, and this affects reaching and grasping accuracy. In our work, we propose a novel method to simultaneously i) estimate the pose of the robot hand, and ii) calibrate the robot kinematic model. This is achieved by combining stereo vision, proprioception, and a 3D computer graphics model of the robot. Notably, the use of GPU programming allows to perform the estimation and calibration in real time during the execution of arm reaching movements. Proprioceptive information is exploited to generate hypotheses about the visual appearance of the hand in the camera images, using the 3D computer graphics model of the robot that includes both kinematic and texture information. These hypotheses are compared with the actual visual input using particle filtering, to obtain both i) the best estimate of the hand pose and ii) a set of joint offsets to calibrate the kinematics of the robot model. We evaluate two different approaches to estimate the 6D pose of the hand from vision (silhouette segmentation and edges extraction) and show experimentally that the pose estimation error is considerably reduced with respect to the nominal robot model. Moreover, the GPU implementation ensures a performance about 3 times faster than the CPU one, allowing real-time operation.",
    issn="1573-0409",
    doi="10.1007/s10846-016-0376-6",
    url="https://doi.org/10.1007/s10846-016-0376-6",
    pdf={vicente16gpuPoseEst_jint.pdf},
}

@ARTICLE{vicente2016frontiers,
    AUTHOR={Vicente, Pedro and Jamone, Lorenzo and Bernardino, Alexandre},   
    TITLE={Online Body Schema Adaptation Based on Internal Mental Simulation and Multisensory Feedback},      
    JOURNAL={Frontiers in Robotics and AI},      
    VOLUME={3},      
    PAGES={7},     
    YEAR={2016},      
    URL={https://www.frontiersin.org/article/10.3389/frobt.2016.00007},       
    DOI={10.3389/frobt.2016.00007},      
    ISSN={2296-9144},
    pdf={pvicente-frontiers2016.pdf},
    ABSTRACT={In this paper, we describe a novel approach to obtain automatic adaptation of the robot body schema and to improve the robot perceptual and motor skills based on this body knowledge. Predictions obtained through a mental simulation of the body are combined with the real sensory feedback to achieve two objectives simultaneously: body schema adaptation and markerless 6D hand pose estimation. The body schema consists of a computer graphics simulation of the robot, which includes the arm and head kinematics (adapted online during the movements) and an appearance model of the hand shape and texture. The mental simulation process generates predictions on how the hand will appear in the robot camera images, based on the body schema and the proprioceptive information (i.e. motor encoders). These predictions are compared to the actual images using Sequential Monte Carlo techniques to feed a particle-based Bayesian estimation method to estimate the parameters of the body schema. The updated body schema will improve the estimates of the 6D hand pose, which is then
    used in a closed-loop control scheme (i.e. visual servoing), enabling precise reaching. We report experiments with the iCub humanoid robot that support the validity of our approach. A number of simulations with precise ground-truth were performed to evaluate the estimation capabilities of the proposed framework. Then, we show how the use of high-performance GPU programming and an edge-based algorithm for visual perception allow for real-time implementation in real world scenarios.}
}

@ARTICLE{10.3389/frobt.2018.00046,
    AUTHOR={Vicente, Pedro and Jamone, Lorenzo and Bernardino, Alexandre},   
    TITLE={Markerless Eye-Hand Kinematic Calibration on the iCub Humanoid Robot},      
    JOURNAL={Frontiers in Robotics and AI},      
    VOLUME={5},      
    PAGES={46},     
    YEAR={2018},      
    URL={https://www.frontiersin.org/article/10.3389/frobt.2018.00046},       
    DOI={10.3389/frobt.2018.00046},      
    ISSN={2296-9144},   
    pdf={pvicente-frontiers2018.pdf},
    ABSTRACT={Humanoid robots are resourceful platforms and can be used in diverse application scenarios.However,  their  high  number  of  degrees  of  freedom  in  (i.e.,  moving  arms,  head  and  eyes) deteriorates the precision of eye-hand coordination.  A good kinematic calibration is often difficult to achieve, due to several factors, e.g., unmodeled deformations of the structure or backlash in the actuators. This is particularly challenging for very complex robots such as the iCub humanoid robot, which has 12 degrees of freedom and cable-driven actuation in the serial chain from the eyes to the hand. The exploitation of real-time robot sensing is of paramount importance to  increase  the  accuracy  of  the  coordination,  for  example,  to  realize  precise  grasping  and manipulation tasks. In this code paper, we propose an online and markerless solution to the eye-hand kinematic calibration of the iCub humanoid robot. We have implemented a sequential Monte Carlo algorithm estimating kinematic calibration parameters (joint offsets) which improve the eye-hand coordination based on the proprioception and vision sensing of the robot. We have shown the usefulness of the developed code and its accuracy on simulation and real-world scenarios. The code is written in C++ and CUDA, where we exploit the GPU to increase the speed of the method. The code is made available online along with a Dataset for testing purposes.}
}