@inproceedings{gcunha2020recpad,
  title={{Active Robot Learning for Efficient Body-Schema Online Adaptation}},
  author={Cunha, Gonçalo and Bernardino, Alexandre and Vicente, Pedro and  and Ribeiro, Ricardo and Moreno, Plínio},
  booktitle={Portuguese Conference on Pattern Recognition (RecPad)},
  year={2020},
  pdf={acunha-recpad2020.pdf},
  ABSTRACT={
    This work proposes an active learning approach for estimating the Denavit-Hartenberg parameters of 7 joints of the iCub arm in a simulation 
    environment, using observations of the end-effector’s pose and knowing the values from proprioceptive sensors. Cost-sensitive active learning, 
    aims to reduce the number of measurements taken and also reduce the total movement performed by the robot while calibrating, thus reducing 
    energy consumption, along with mechanical fatigue and wear. The estimation of the arm’s parameters is done using the Extended Kalman Filter 
    and the active exploration is guided by the A-Optimality criterion. The results show cost-sensitive active learning can perform similarly to 
    the straightforward active learning approach, while reducing significantly the necessary movement.
  },
}
@INPROCEEDINGS{mnascimento2020icarsc,
  author={Miguel {Nascimento} and Pedro {Vicente} and Alexandre {Bernardino}},
  booktitle={IEEE International Conference on Autonomous Robot Systems and Competitions (ICARSC)}, 
  title={2D Visual Servoing meets Rapidly-exploring Random Trees for collision avoidance}, 
  year={2020},
  pages={227-232},
  doi={10.1109/ICARSC49921.2020.9096133},
  pdf={mnascimento2020icarsc.pdf},
  ABSTRACT = {
    Visual Servoing is a well-known subject in robotics. However, there are still some challenges on the visual control of robots for applications 
    in human environments. In this article, we propose a method for path planning and correction of kinematic errors using visual servoing. 
    3D information provided by external cameras will be used for segmenting the environment and detecting the obstacles in the scene. 
    Rapidly-exploring Random Trees are then used to calculate a path through the obstacles to a given, previously calculated, end-effector goal pose. 
    This allows for model-free path planning for cluttered environments by using a point cloud representation of the environment. 
    The proposed path is then followed by the robot in open-loop. Error correction is performed near the goal pose by using real-time calculated 
    image features as control points for an Image-Based Visual Servoing controller that drives the end-effector towards the desired goal pose. 
    With this method, we intend to achieve the navigation of a robotic arm through a cluttered environment towards a goal pose with error correction 
    performed at the end of the trajectory to mitigate both the weaknesses of Image Based Visual Servoing and of open-loop trajectory following. 
    We made several experiments in order to validate our approach by evaluating each individual main component (environment segmentation, trajectory 
    calculation and error correction through visual servoing) of our solution. Furthermore, our solution was implemented in ROS using the Baxter 
    Research Robot.
  }
}
%%%%% incomplete  %%%%%
@inproceedings{adehban2019icdl,
  title={{Robotic Interactive Physics Parameters Estimator (RIPPE)}},
  author={Dehban, Atabak and Cardoso, Carlos and Vicente, Pedro and Bernardino, Alexandre and Santos-Victor, José},
  booktitle={Joint IEEE International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob)},
  year={2019},
  organization={IEEE},
  pdf={adehban-icdl2019.pdf},
  ABSTRACT={
    The ability to reason about natural laws of an environment directly contributes to successful performance in it.
    In this work, we present RIPPE, a framework that allows a robot to leverage existing physics simulators as its 
    knowledge base for learning interactions with in-animate objects. To achieve this, the robot needs to initially
    interact with its surrounding environment and observe the effects of its behaviours. Relying on the simulator
    to efficiently solve the partial differential equations describing these physical interactions, the robot 
    infers consistent physical parameters of its surroundings by repeating the same actions in simulation and 
    evaluate how closely they match its real observations. The learning process is performed using Bayesian 
    Optimisation techniques to sample efficiently the parameter space. We assess the utility of these inferred
    parameters by measuring how well they can explain physical interactions using previously unseen actions and tools.
  },
}

%%%%% incomplete  %%%%%
@inproceedings{jcastanheira2018iros,
  title={{Finding safe 3D robot grasps through efficient haptic exploration with unscented Bayesian optimization and collision penalty}},
  author={Castanheira, João and Vicente, Pedro and Martinez-Cantin, Ruben and Jamone, Lorenzo and Bernardino, Alexandre},
  booktitle={IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  year={2018},
  organization={IEEE},
  pdf={jcastanheira-iros2018.pdf},
  ABSTRACT={
    Robust grasping is a major, and still unsolved,
    problem in robotics. Information about the 3D shape of an
    object can be obtained either from prior knowledge (e.g.,
    accurate models of known objects or approximate models of
    familiar objects) or real-time sensing (e.g., partial point clouds
    of unknown objects) and can be used to identify good potential
    grasps. However, due to modeling and sensing inaccuracies,
    local exploration is often needed to refine such grasps and
    successfully apply them in the real world. The recently proposed
    unscented Bayesian optimization technique can make such exploration safer by selecting grasps that are robust to uncertainty
    in the input space (e.g., inaccuracies in the grasp execution).
    Extending our previous work on 2D optimization, in this paper
    we propose a 3D haptic exploration strategy that combines
    unscented Bayesian optimization with a novel collision penalty
    heuristic to find safe grasps in a very efficient way: while by
    augmenting the search-space to 3D we are able to find better
    grasps, the collision penalty heuristic allows us to do so without
    increasing the number of exploration steps.
  },
}

%%%%% incomplete  %%%%%
@inproceedings{rzenha2018icdl,
  title={{Incremental adaptation of a robot body schema based on touch events}},
  author={Zenha, Rodrigo and Vicente, Pedro and Jamone, Lorenzo and Bernardino, Alexandre},
  booktitle={Joint IEEE International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob)},
  year={2018},
  organization={IEEE},
  pdf={rzenha_icdl2018.pdf},
  ABSTRACT={
    The term ‘body schema’ refers to a computational
    representation of a physical body; the neural representation of
    a human body, or the numerical representation of a robot body.
    In both humans and robots, such a representation is crucial to
    accurately control body movements. While humans learn and
    continuously adapt their body schema based on multimodal
    perception and neural plasticity, robots are typically assigned
    with a fixed analytical model (e.g., the robot kinematics) which
    describes their bodies. However, there are always discrepancies
    between a model and the real robot, and they vary over
    time, thus affecting the accuracy of movement control. In
    this work, we equip a humanoid robot with the ability to
    incrementally estimate such model inaccuracies by touching
    known planar surfaces (e.g., walls) in its vicinity through
    motor babbling exploration, effectively adapting its own body
    schema based on the contact information alone. The problem
    is formulated as an adaptive parameter estimation (Extended
    Kalman Filter) which makes use of planar constraints obtained
    at each contact detection. We compare different incremental
    update methods through an extensive set of experiments with a
    realistic simulation of the iCub humanoid robot, showing that
    the model inaccuracies can be reduced by more than 80%.
  },
}

%%%%% incomplete  %%%%%
@inproceedings{cauli2018autonomous,
  title={{Autonomous table-cleaning from kinesthetic demonstrations using Deep Learning}},
  author={Cauli, Nino and Vicente, Pedro and Kim, Jaeseok and Damas, Bruno and Bernardino, Alexandre and Cavallo, Filippo and Santos-Victor, Jos{\'e}},
  booktitle={Joint IEEE International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob)},
  year={2018},
  organization={IEEE},
  pdf={ncauli_icdl2018.pdf},
  ABSTRACT={
    We address the problem of teaching a robot how
    to autonomously perform table-cleaning tasks in a robust way.
    In particular, we focus on wiping and sweeping a table with
    a tool (e.g., a sponge). For the training phase, we use a set
    of kinestethic demonstrations performed over a table. The
    recorded 2D table-space trajectories, together with the images
    acquired by the robot, are used to train a deep convolutional
    network that automatically learns the parameters of a Gaussian
    Mixture Model that represents the hand movement. After the
    learning stage, the network is fed with the current image show-
    ing the location/shape of the dirt or stain to clean. The robot
    is able to perform cleaning arm-movements, obtained through
    Gaussian Mixture Regression using the mixture parameters
    provided by the network. Invariance to the robot posture is
    achieved by applying a plane-projective transformation before
    inputting the images to the neural network; robustness to
    illumination changes and other disturbances is increased by
    considering an augmented data set. This improves the general-
    ization properties of the neural network, enabling for instance
    its use with the left arm after being trained using trajectories
    acquired with the right arm. The system was tested on the
    iCub robot generating a cleaning behaviour similar to the one
    of human demonstrators.
  }
}

@inproceedings{kim2018icub,
  title={{``iCub, clean the table!'' A robot learning from demonstration approach using deep neural networks}},
  author={Kim, Jaeseok and Cauli, Nino and Vicente, Pedro and Damas, Bruno and Cavallo, Filippo and Santos-Victor, Jos{\'e}},
  booktitle={IEEE International Conference on Autonomous Robot Systems and Competitions (ICARSC)},
  pages={3--9},
  year={2018},
  organization={IEEE},
  pdf={jkim-icarsc2018.pdf},
  bestp={Best Conference Paper Award},
  doi={10.1109/ICARSC.2018.8374152}, 
  ABSTRACT={
    Autonomous service robots have become a key
    research topic in robotics, particularly for household chores.
    A typical home scenario is highly unconstrained and a service
    robot needs to adapt constantly to new situations. In this
    paper, we address the problem of autonomous cleaning tasks
    in uncontrolled environments. In our approach, a human
    instructor uses kinestethic demonstrations to teach a robot how
    to perform different cleaning tasks on a table. Then, we use Task
    Parametrized Gaussian Mixture Models (TP-GMMs) to encode
    the demonstrations variability, while providing appropriate
    generalization abilities. TP-GMMs extend Gaussian Mixture
    Models with an auxiliary set of reference frames, in order to ex-
    trapolate the demonstrations to different task parameters such
    as movement locations, amplitude or orientations. However, the
    reference frames (that parametrize TP-GMMs) can be very
    difficult to extract in practice, as it may require segmenting the
    cluttered images of the working table-top. Instead, in this work
    the reference frames are automatically extracted from robot
    camera images, using a deep neural network that was trained
    during human demonstrations of a cleaning task. This approach
    has two main benefits: (i) it takes the human completely out of
    the loop while performing complex cleaning tasks; and (ii) the
    network is able to identify the specific task to be performed
    directly from image data, thus also enabling automatic task
    selection from a set of previously demonstrated tasks. The
    system was implemented on the iCub humanoid robot. During
    the tests, the robot was able to successfully clean a table with
    two different types of dirt (wiping a marker’s scribble or
    sweeping clusters of lentils).
  } 
}

@INPROCEEDINGS{saponaro2017icdl, 
  author={G. Saponaro and P. Vicente and A. Dehban and L. Jamone and A. Bernardino and J. Santos-Victor}, 
  booktitle={Joint IEEE International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob)}, 
  title={{Learning at the ends: From hand to tool affordances in humanoid robots}}, 
  year={2017}, 
  volume={}, 
  number={}, 
  pages={331-337}, 
  keywords={end effectors;humanoid robots;human-robot interaction;learning (artificial intelligence);learning systems;tool affordances;humanoid robots;open challenges;unpredictable human environment;object affordances;possible world interactions;training data;manipulative robot;external objects;similar end-effectors;sensorimotor experiences;sensorimotor skills;bare hands;visual motor imagination mechanism;different hand postures;probabilistic model;unseen tools;tool selection tasks;iCub humanoid robot;collected sensorimotor data;hand posture affordances dataset;possible human-made tools;Tools;Robot sensing systems;Visualization;Humanoid robots;Solid modeling;Shape}, 
  doi={10.1109/DEVLRN.2017.8329826}, 
  ISSN={2161-9484}, 
  pdf={gsaponaro-icdlepirob2017.pdf},
  month={Sept},
  ABSTRACT={
    One of the open challenges in designing robots that
    operate successfully in the unpredictable human environment
    is how to make them able to predict what actions they can
    perform on objects, and what their effects will be, i.e., the ability
    to perceive object affordances. Since modeling all the possible
    world interactions is unfeasible, learning from experience is
    required, posing the challenge of collecting a large amount
    of experiences (i.e., training data). Typically, a manipulative
    robot operates on external objects by using its own hands (or
    similar end-effectors), but in some cases the use of tools may be
    desirable; nevertheless, it is reasonable to assume that while a
    robot can collect many sensorimotor experiences using its own
    hands, this cannot happen for all possible human-made tools.
    Therefore, in this paper we investigate the developmental
    transition from hand to tool affordances: what sensorimotor
    skills that a robot has acquired with its bare hands can be
    employed for tool use? By employing a visual and motor
    imagination mechanism to represent different hand postures
    compactly, we propose a probabilistic model to learn hand
    affordances, and we show how this model can generalize to
    estimate the affordances of previously unseen tools, ultimately
    supporting planning, decision-making and tool selection tasks
    in humanoid robots. We present experimental results with the
    iCub humanoid robot, and we publicly release the collected
    sensorimotor data in the form of a hand posture affordances
    dataset.
  }
}

@INPROCEEDINGS{vicente2017icra, 
  author={P. Vicente and L. Jamone and A. Bernardino}, 
  booktitle={IEEE International Conference on Robotics and Automation (ICRA)}, 
  title={{Towards markerless visual servoing of grasping tasks for humanoid robots}}, 
  year={2017}, 
  volume={}, 
  number={}, 
  pages={3811-3816}, 
  keywords={computer graphics;error analysis;humanoid robots;manipulator kinematics;optimisation;pose estimation;robot vision;stereo image processing;3D model-based stereo-vision algorithm;edge-based distance transform metric;eye-to-hand kinematics configuration;grasping task visual servoing;hand-to-object relative pose measurement;humanoid grasping tasks;iCub robot;kinematic calibration errors;position estimation;robot arm-hand internal computer-graphic model;robust robot pose estimation;synthetically generated images;vision-based grasping;visual feedback;Calibration;Grasping;Humanoid robots;Kinematics;Solid modeling;Visualization}, 
  doi={10.1109/ICRA.2017.7989441}, 
  ISSN={}, 
  pdf={pvicente_ICRA2017.pdf},
  month={May},
  ABSTRACT={
    Vision-based grasping for humanoid robots is a
    challenging problem due to a multitude of factors. First,
    humanoid robots use an “eye-to-hand” kinematics configuration
    that, on the contrary to the more common “eye-in-hand”
    configuration, demands a precise estimate of the position of the
    robot’s hand. Second, humanoid robots have a long kinematic
    chain from the eyes to the hands, prone to accumulate the
    calibration errors of the kinematics model, which offsets the
    measured hand-to-object relative pose from the real one. In
    this paper, we propose a method able to solve these two
    issues jointly. A robust pose estimation of the robot’s hand
    is achieved via a 3D model-based stereo-vision algorithm, using
    an edge-based distance transform metric and synthetically
    generated images of a robot’s arm-hand internal computer-
    graphics model (kinematics and appearance). Then, a particle-
    based optimisation method adapts on-line the robot’s internal
    model to match the real and the synthetically generated images,
    effectively compensating the kinematics calibration errors. We
    evaluate the proposed approach using a position-based visual-
    servoing method on the iCub robot, showing the importance of
    the continuous visual feedback in humanoid grasping tasks.
  }
}

@INPROCEEDINGS{vicente2017icarsc, 
  author={P. Vicente and A. Bernardino}, 
  booktitle={IEEE International Conference on Autonomous Robot Systems and Competitions (ICARSC)}, 
  title={{Wedding robotics: A case study}}, 
  year={2017}, 
  volume={}, 
  number={}, 
  pages={140-145}, 
  keywords={mobile robots;robot behaviour;robot performance;social robot;technical details;wedding guest reactions;wedding ring bearer;wedding robotics;Conferences;Context;Humanoid robots;Robot sensing systems;Service robots;Software;case study;human-robot interaction;humanoid robot;social experiment;social robotics}, 
  doi={10.1109/ICARSC.2017.7964066}, 
  pdf={pvicente-icarsc2017.pdf},
  month={April},
  ABSTRACT={
    In this work, we propose to study a social robot in a
    wedding context, where it plays the role of a wedding ring bearer.
    We focus on the interaction with the audience, their expectations,
    and reactions, rather than in technical details. We collect data
    from 121 individuals belonging to two different groups, those
    who have seen the robot behaviour (live or recorded versions) and
    those who did not see the robot performance. We divide the study
    into three parts: i) the reactions of the guests at the wedding,
    ii) a comparison between subjects which were exposed or not to
    the robot behaviour, and iii) a within-subjects experiment where
    after filling a survey, they are asked to see the recorded robot
    behaviour. The guests reacted positively to the experiment. The
    robot was considered likeable, lively and safe by the majority
    of the participants in the study. The group that observed the
    robot’s behaviour had a better opinion on the use of robots in
    wedding ceremonies than the group that did not observe the
    experience. This may suggest that a higher presence of robots in
    social activities will increase the acceptance of robots in society.
    Index Terms—humanoid robot, social robotics, human-robot
    interaction, social experiment, case study.
  }
}

@inproceedings{vicente2015icarsc, 
  author={Pedro Vicente and Ricardo Ferreira and Lorenzo Jamone and Alexandre Bernardino}, 
  booktitle={IEEE International Conference on Autonomous Robot Systems and Competitions (ICARSC)}, 
  title={{GPU-Enabled Particle Based Optimization for Robotic-Hand Pose Estimation and Self-Calibration}}, 
  year={2015}, 
  pages={3-8}, 
  keywords={CAD;control engineering computing;graphics processing units;humanoid robots;manipulator kinematics;optimisation;particle filtering (numerical methods);pose estimation;robot vision;solid modelling;3D CAD model;3D hand pose estimation method;GPU-enabled particle based optimization;GPU-enabled vision;Unity Technologies;computer graphics model;game engine;humanoid robots;kinematic chains;particle filter techniques;proprioceptive sensing;robot arm;robotic-hand pose estimation;robotic-hand self-calibration;visual hypotheses;visual sensing;Computational modeling;Estimation;Graphics processing units;Joints;Kinematics;Robots;Visualization;3D model based tracking;GPU;humanoid robot;reaching;robot self-calibration;robotic-hand pose estimation}, 
  doi={10.1109/ICARSC.2015.25}, 
  pdf={pvicente-ICARSC15.pdf},
  month={April},
  ABSTRACT={
    Humanoid robots have complex kinematic chains
    that are difficult to model with the precision required to reach
    and/or grasp objects properly. In this paper we propose a GPU-
    enabled vision based 3D hand pose estimation method that
    runs during robotic reaching tasks to calibrate in real time the
    kinematic chain of the robot arm. This is achieved by combining:
    i) proprioceptive and visual sensing; and ii) a kinematic and
    computer graphics model of the system. We use proprioceptive
    input to create visual hypotheses about the hand appearance in
    the image using a 3D CAD model inside the game engine from
    Unity Technologies. These hypotheses are compared with the
    actual visual input using particle filter techniques. The outcome
    of this processing is the best hypothesis for the hand pose and a
    set of joint offsets to calibrate the arm. We tested our approach
    in a simulation environment and verified that the angular error is
    reduced 3 times and the position error about 12 times comparing
    with the non-calibrated case (proprioception only). The used
    GPU implementation techniques ensures a performance 2.5 times
    faster than performing the computations on the CPU.
    Index Terms—humanoid robot, robotic-hand pose estimation,
    robot self-calibration, 3D model based tracking, GPU, reaching.
  }
}

@inproceedings{vicente2014icdl, 
  author={Pedro Vicente and Ricardo Ferreira and Lorenzo Jamone and Alexandre Bernardino}, 
  booktitle={Joint IEEE International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob)}, 
  title={{Eye-hand online adaptation during reaching tasks in a humanoid robot}}, 
  year={2014}, 
  pages={175-180}, 
  organization={IEEE},
  keywords={CAD;cameras;dexterous manipulators;humanoid robots;image sensors;manipulator kinematics;particle filtering (numerical methods);pose estimation;robot vision;solid modelling;ballistic open-loop phase;cameras;eye-hand online adaptation;humanoid robot arm kinematics;kinematic errors;particle filter;proprioceptive sensors;reaching tasks;robot hand 3D CAD model;vision based 3D hand pose estimation method;visual sensors;Cameras;Joints;Kinematics;Robot vision systems;Visualization;3D model based tracking;Online adaptation;humanoid robot;internal model learning;reaching}, 
  doi={10.1109/DEVLRN.2014.6982978}, 
  pdf={vicente14icdlEyeHand.pdf},
  month={Oct},
  ABSTRACT={
    In this paper we propose a method for the online
    adaptation of a humanoid robot’s arm kinematics, using its
    visual and proprioceptive sensors. A typical reaching movement
    starts with a ballistic open-loop phase to bring the hand to the
    vicinity of the object. During this phase, as soon as the hand of
    the robot enters the field of view of one of its cameras, a vision
    based 3D hand pose estimation method feeds a particle filter that
    gradually adjusts the arm kinematics’ parameters. Our method
    makes use of a 3D CAD model of the robot hand (geometry
    and texture) whose predicted position in the image is compared
    at each time step with the cameras’ incoming information.
    When the hand gets close to the object, the kinematic errors
    have reduced significantly and a better control of grasping can
    eventually be achieved. We have tested the method both in
    simulation and with the real robot and verify error decreases
    by a factor of 3 during a typical reaching time span.
    Index Terms—Online adaptation, internal model learning, 3D
    model based tracking, reaching, humanoid robot.
  }
}


